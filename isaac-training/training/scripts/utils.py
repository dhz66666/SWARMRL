# import torch
# import torch.nn as nn
# import wandb
# import numpy as np
# from typing import Iterable, Union
# from tensordict.tensordict import TensorDict
# from omni_drones.utils.torchrl import RenderCallback
# from torchrl.envs.utils import ExplorationType, set_exploration_type

# class ValueNorm(nn.Module):
#     def __init__(
#         self,
#         input_shape: Union[int, Iterable],
#         beta=0.995,
#         epsilon=1e-5,
#     ) -> None:
#         super().__init__()

#         self.input_shape = (
#             torch.Size(input_shape)
#             if isinstance(input_shape, Iterable)
#             else torch.Size((input_shape,))
#         )
#         self.epsilon = epsilon
#         self.beta = beta

#         self.running_mean: torch.Tensor
#         self.running_mean_sq: torch.Tensor
#         self.debiasing_term: torch.Tensor
#         self.register_buffer("running_mean", torch.zeros(input_shape))
#         self.register_buffer("running_mean_sq", torch.zeros(input_shape))
#         self.register_buffer("debiasing_term", torch.tensor(0.0))

#         self.reset_parameters()

#     def reset_parameters(self):
#         self.running_mean.zero_()
#         self.running_mean_sq.zero_()
#         self.debiasing_term.zero_()

#     def running_mean_var(self):
#         debiased_mean = self.running_mean / self.debiasing_term.clamp(min=self.epsilon)
#         debiased_mean_sq = self.running_mean_sq / self.debiasing_term.clamp(
#             min=self.epsilon
#         )
#         debiased_var = (debiased_mean_sq - debiased_mean**2).clamp(min=1e-2)
#         return debiased_mean, debiased_var

#     @torch.no_grad()
#     def update(self, input_vector: torch.Tensor):
#         assert input_vector.shape[-len(self.input_shape) :] == self.input_shape
#         dim = tuple(range(input_vector.dim() - len(self.input_shape)))
#         batch_mean = input_vector.mean(dim=dim)
#         batch_sq_mean = (input_vector**2).mean(dim=dim)

#         weight = self.beta

#         self.running_mean.mul_(weight).add_(batch_mean * (1.0 - weight))
#         self.running_mean_sq.mul_(weight).add_(batch_sq_mean * (1.0 - weight))
#         self.debiasing_term.mul_(weight).add_(1.0 * (1.0 - weight))

#     def normalize(self, input_vector: torch.Tensor):
#         assert input_vector.shape[-len(self.input_shape) :] == self.input_shape
#         mean, var = self.running_mean_var()
#         out = (input_vector - mean) / torch.sqrt(var)
#         return out

#     def denormalize(self, input_vector: torch.Tensor):
#         assert input_vector.shape[-len(self.input_shape) :] == self.input_shape
#         mean, var = self.running_mean_var()
#         out = input_vector * torch.sqrt(var) + mean
#         return out

# def make_mlp(num_units):
#     layers = []
#     for n in num_units:
#         layers.append(nn.LazyLinear(n))
#         layers.append(nn.LeakyReLU())
#         layers.append(nn.LayerNorm(n))
#     return nn.Sequential(*layers)

# class IndependentNormal(torch.distributions.Independent):
#     arg_constraints = {"loc": torch.distributions.constraints.real, "scale": torch.distributions.constraints.positive} 
#     def __init__(self, loc, scale, validate_args=None):
#         scale = torch.clamp_min(scale, 1e-6)
#         base_dist = torch.distributions.Normal(loc, scale)
#         super().__init__(base_dist, 1, validate_args=validate_args)

# class IndependentBeta(torch.distributions.Independent):
#     arg_constraints = {"alpha": torch.distributions.constraints.positive, "beta": torch.distributions.constraints.positive}

#     def __init__(self, alpha, beta, validate_args=None):
#         beta_dist = torch.distributions.Beta(alpha, beta)
#         super().__init__(beta_dist, 1, validate_args=validate_args)

# class Actor(nn.Module):
#     def __init__(self, action_dim: int) -> None:
#         super().__init__()
#         self.actor_mean = nn.LazyLinear(action_dim)
#         self.actor_std = nn.Parameter(torch.zeros(action_dim)) 
    
#     def forward(self, features: torch.Tensor):
#         loc = self.actor_mean(features)
#         scale = torch.exp(self.actor_std).expand_as(loc)
#         return loc, scale

# class BetaActor(nn.Module):
#     def __init__(self, action_dim: int) -> None:
#         super().__init__()
#         self.alpha_layer = nn.LazyLinear(action_dim)
#         self.beta_layer = nn.LazyLinear(action_dim)
#         self.alpha_softplus = nn.Softplus()
#         self.beta_softplus = nn.Softplus()
    
#     def forward(self, features: torch.Tensor):
#         alpha = 1. + self.alpha_softplus(self.alpha_layer(features)) + 1e-6
#         beta = 1. + self.beta_softplus(self.beta_layer(features)) + 1e-6
#         # print("alpha: ", alpha)
#         # print("beta: ", beta)
#         return alpha, beta

# class GAE(nn.Module):
#     def __init__(self, gamma, lmbda):
#         super().__init__()
#         self.register_buffer("gamma", torch.tensor(gamma))
#         self.register_buffer("lmbda", torch.tensor(lmbda))
#         self.gamma: torch.Tensor
#         self.lmbda: torch.Tensor
    
#     def forward(
#         self, 
#         reward: torch.Tensor, 
#         terminated: torch.Tensor, 
#         value: torch.Tensor, 
#         next_value: torch.Tensor
#     ):
#         num_steps = terminated.shape[1]
#         advantages = torch.zeros_like(reward)
#         not_done = 1 - terminated.float()
#         gae = 0
#         for step in reversed(range(num_steps)):
#             delta = (
#                 reward[:, step] 
#                 + self.gamma * next_value[:, step] * not_done[:, step] 
#                 - value[:, step]
#             )
#             advantages[:, step] = gae = delta + (self.gamma * self.lmbda * not_done[:, step] * gae) 
#         returns = advantages + value
#         return advantages, returns

# def make_batch(tensordict: TensorDict, num_minibatches: int):
#     tensordict = tensordict.reshape(-1) 
#     perm = torch.randperm(
#         (tensordict.shape[0] // num_minibatches) * num_minibatches,
#         device=tensordict.device,
#     ).reshape(num_minibatches, -1)
#     for indices in perm:
#         yield tensordict[indices]

# # @torch.no_grad()
# # def evaluate(env, policy, cfg, seed: int=0, exploration_type: ExplorationType=ExplorationType.MEAN):
# #     # 1. ç¡®ä¿ç¯å¢ƒå’Œç­–ç•¥è¢«æ˜¾å¼æ¨å‘æŒ‡å®šè®¾å¤‡
# #     env.to(cfg.device) 
# #     policy.to(cfg.device)
    
# #     env.enable_render(True)
# #     env.eval()
# #     env.set_seed(seed)

# #     # 2. è·å–åˆå§‹å¸§å¹¶å¼ºåˆ¶æŒ‡å®šè®¾å¤‡
# #     # å³ä½¿ç¯å¢ƒé‡Œæ”¹äº†ï¼Œè¿™é‡Œæ‰‹åŠ¨ .to() ä¹Ÿæ˜¯åŒé‡ä¿é™©
# #     td = env.reset().to(cfg.device) 

# #     render_callback = RenderCallback(interval=2)
    
# #     # 3. æ‰§è¡Œ 10 æ­¥å¿«é€Ÿæµ‹è¯•å¾ªç¯
# #     max_steps = 10 
# #     traj_list = []
    
# #     with set_exploration_type(exploration_type):
# #         for _ in range(max_steps):
# #             # ç¡®ä¿è¾“å…¥ Policy å‰ TensorDict å¸¦æœ‰æ­£ç¡®çš„ device å±æ€§
# #             td = td.to(cfg.device) 
            
# #             # æ‰§è¡ŒåŠ¨ä½œ
# #             td = env.step(policy(td))
            
# #             # æ‰§è¡Œæ¸²æŸ“å›è°ƒ
# #             render_callback(env)
            
# #             # å…‹éš†å¹¶å­˜å‚¨ï¼Œç¡®ä¿æ¯ä¸€å¸§çš„è®¾å¤‡å±æ€§éƒ½è¢«ä¿ç•™
# #             traj_list.append(td.clone())
            
# #         # 4. æ‰‹åŠ¨å †å 
# #         trajs = torch.stack(traj_list, dim=1) 
# #         # --- è¯Šæ–­ä»£ç å¼€å§‹ ---
# #     print("-" * 30)
# #     print(f"DEBUG: trajs shape: {trajs.shape}") # æ•´ä½“æ•°æ®å½¢çŠ¶
# #     stats_td = trajs.get(("next", "stats"))
# #     print(f"DEBUG: stats_keys: {stats_td.keys()}") # ç»Ÿè®¡é¡¹æœ‰å“ªäº›
# #     test_key = list(stats_td.keys())[0]
# #     print(f"DEBUG: sample stats item ({test_key}) shape: {stats_td[test_key].shape}")
# #     print(f"DEBUG: done shape: {trajs.get(('next', 'done')).shape}")
# #     print("-" * 30)
# #     # --- è¯Šæ–­ä»£ç ç»“æŸ ---
# #     # 5. åç»­å¤„ç†é€»è¾‘
# #     env.enable_render(not cfg.headless)
# #     env.train()
# #     env.reset() # è¯„ä¼°åé‡ç½®ç¯å¢ƒçŠ¶æ€
# #     # --- ä»¥ä¸‹ä¸ºåŸæœ‰çš„æ•°æ®ç»Ÿè®¡é€»è¾‘ï¼Œæ— éœ€ä¿®æ”¹ ---
# #     done = trajs.get(("next", "done")) 
# #     # æ‰¾åˆ°æ¯ä¸ªç¯å¢ƒç¬¬ä¸€æ¬¡ done çš„ç´¢å¼•
# #     first_done = torch.argmax(done.long(), dim=1).cpu()

# # # 1. æ‰¾åˆ°æ¯ä¸ªç¯å¢ƒç¬¬ä¸€æ¬¡ done çš„ç´¢å¼• (å½¢çŠ¶: [num_envs])
# # # 1. è·å– Done ä¿¡å·å¹¶å¤„ç†ç»´åº¦
# #     # done åŸå§‹å½¢çŠ¶é€šå¸¸æ˜¯ [128, 2, 10, 1] (env, agent, time, 1)
# #     done = trajs.get(("next", "done")).cpu()
    
# #     # æŠŠå®ƒå‹å¹³å‰ä¸¤ä¸ªç»´åº¦ï¼Œå˜æˆ [256, 10]
# #     # ç„¶åæ‰¾åˆ°æ¯ä¸€æ¶é£æœºç¬¬ä¸€æ¬¡ done çš„æ­¥æ•°ç´¢å¼•
# #     first_done = torch.argmax(done.squeeze(-1).float(), dim=-1).flatten() 

# #     def take_first_episode(tensor: torch.Tensor):
# #         # t åŸå§‹å½¢çŠ¶å¯èƒ½æ˜¯ [128, 2, 10, ...]
# #         t = tensor.cpu()
# #         shape = t.shape
        
# #         # ğŸ’¥ å…³é”®ç‚¹ï¼šæŠŠ [128, 2] åˆå¹¶æˆ [256]ï¼Œè®©å®ƒå’Œ first_done çš„ 256 å¯¹é½
# #         # å˜å½¢åä¸º [256, 10, ...]
# #         t = t.reshape(-1, shape[2], *shape[3:])
        
# #         # è®¡ç®—éœ€è¦è¡¥é½çš„ç»´åº¦ï¼ˆæ¯”å¦‚ stats æ˜¯æ ‡é‡è¿˜æ˜¯å‘é‡ï¼‰
# #         needed_dims = t.ndim - first_done.ndim 
        
# #         # è¿™é‡Œçš„ first_done å½¢çŠ¶æ˜¯ [256]ï¼Œview ä¹‹åæ˜¯ [256, 1, 1...]
# #         indices = first_done.view(first_done.shape[0], *((1,) * needed_dims))
        
# #         # ä»æ—¶é—´è½´ (dim=1) æå–æ•°æ®
# #         res = torch.take_along_dim(t, indices, dim=1)
# #         return res.reshape(-1)

# #     # 2. åº”ç”¨åˆ°æ‰€æœ‰çš„ stats é¡¹ä¸Š
# #     stats_dict = trajs.get(("next", "stats"))
# #     traj_stats = {
# #         k: take_first_episode(v)
# #         for k, v in stats_dict.items()
# #     }

# #     # 3. è®¡ç®—æœ€ç»ˆå‡å€¼å‘é€ç»™ wandb
# #     info = {
# #         "eval/stats." + k: torch.mean(v.float()).item() 
# #         for k, v in traj_stats.items()
# #     }
# #     return info
# @torch.no_grad()
# def evaluate(
#     env,
#     policy,
#     cfg,
#     seed: int=0, 
#     exploration_type: ExplorationType=ExplorationType.MEAN
# ):
#     # 1. å¼ºåˆ¶ç‰©ç†å’Œç­–ç•¥å¯¹é½åˆ° GPU
#     env.to(cfg.device) 
#     policy.to(cfg.device)
#     env.enable_render(True)
#     env.eval()
#     env.set_seed(seed)

#     # 2. è¡¥ä¸ï¼šæ‰‹åŠ¨é‡ç½®å¹¶ç¡®ä¿åˆå§‹ TensorDict æºå¸¦ device å±æ€§
#     # è¿™è§£å†³äº† "got None" çš„åˆå§‹å¸§é—®é¢˜
#     td = env.reset().to(cfg.device) 

#     render_callback = RenderCallback(interval=2)
    
#     # 3. è·å–æ­¥æ•°ï¼šç›´æ¥ä»ä½ åœ¨ YAML é‡Œå®šä¹‰çš„å˜é‡è¯»å–
#     max_steps = env.max_episode_length 
#     traj_list = []
    
#     with set_exploration_type(exploration_type):
#         # ğŸ’¥ å…³é”®æ”¹åŠ¨ï¼šä½¿ç”¨æ‰‹åŠ¨å¾ªç¯ä»£æ›¿ rolloutï¼Œç¡®ä¿æ¯ä¸€å¸§éƒ½å¼ºåˆ¶å¯¹é½è®¾å¤‡
#         for _ in range(max_steps):
#             # ç¡®ä¿è¾“å…¥æ•°æ®ä¸€å®šåœ¨ GPU 
#             td = td.to(cfg.device) 
            
#             # ç¯å¢ƒæ­¥è¿›
#             td = env.step(policy(td))
            
#             # æ‰§è¡Œæ¸²æŸ“ï¼ˆä¹‹å‰å·²æ”¹è¿‡ Callback ç¡®ä¿ frame ä¸º numpyï¼‰
#             render_callback(env)
            
#             # å­˜å…¥åˆ—è¡¨ (clone ä¿è¯æ•°æ®ç‹¬ç«‹æ€§)
#             traj_list.append(td.clone())
            
#         # 4. æ‰‹åŠ¨æ‰§è¡Œå †å ï¼šæ¨¡æ‹Ÿ rollout çš„è¾“å‡ºç»“æ„ [env, time, agent, ...]
#         trajs = torch.stack(traj_list, dim=1) 

#     env.enable_render(not cfg.headless)
#     env.reset()
    
#     # --- ä¸‹é¢æ˜¯å·²ç»éªŒè¯è¿‡çš„å¤šæ™ºèƒ½ä½“å…¼å®¹ç»Ÿè®¡é€»è¾‘ ---
#     done = trajs.get(("next", "done")).cpu()
#     # åˆå¹¶å‰ä¸¤ä¸ªç»´åº¦ [128, 2] -> [256] ä»¥åŒ¹é…ä½ çš„é£æœºæ€»æ•°
#     first_done = torch.argmax(done.squeeze(-1).float(), dim=-1).flatten() 

#     def take_first_episode(tensor: torch.Tensor):
#         t = tensor.cpu()
#         shape = t.shape
#         # åŒæ ·åˆå¹¶å‰ä¸¤ä¸ªç»´åº¦ï¼Œå˜æˆ [256, max_steps, ...]
#         t = t.reshape(-1, shape[2], *shape[3:])
#         needed_dims = t.ndim - first_done.ndim
#         indices = first_done.view(first_done.shape[0], *((1,) * needed_dims))
#         return torch.take_along_dim(t, indices, dim=1).reshape(-1)

#     traj_stats = {
#         k: take_first_episode(v)
#         for k, v in trajs.get(("next", "stats")).items()
#     }

#     info = {
#         "eval/stats." + k: torch.mean(v.float()).item() 
#         for k, v in traj_stats.items()
#     }

#     # è§†é¢‘è®°å½•
#     info["recording"] = wandb.Video(
#         render_callback.get_video_array(axes="t c h w"), 
#         fps=0.5 / (cfg.sim.dt * cfg.sim.substeps), 
#         format="mp4"
#     )
    
#     env.train()
#     return info

# # def vec_to_new_frame(vec, goal_direction):
# #     if (len(vec.size()) == 1):
# #         vec = vec.unsqueeze(0)
# #     # print("vec: ", vec.shape)

# #     # goal direction x
# #     goal_direction_x = goal_direction / goal_direction.norm(dim=-1, keepdim=True)
# #     z_direction = torch.tensor([0, 0, 1.], device=vec.device)
    
# #     # goal direction y
# #     goal_direction_y = torch.cross(z_direction.expand_as(goal_direction_x), goal_direction_x)
# #     goal_direction_y /= goal_direction_y.norm(dim=-1, keepdim=True)
    
# #     # goal direction z
# #     goal_direction_z = torch.cross(goal_direction_x, goal_direction_y)
# #     goal_direction_z /= goal_direction_z.norm(dim=-1, keepdim=True)

# #     n = vec.size(0)
# #     if len(vec.size()) == 3:
# #         vec_x_new = torch.bmm(vec.view(n, vec.shape[1], 3), goal_direction_x.view(n, 3, 1)) 
# #         vec_y_new = torch.bmm(vec.view(n, vec.shape[1], 3), goal_direction_y.view(n, 3, 1))
# #         vec_z_new = torch.bmm(vec.view(n, vec.shape[1], 3), goal_direction_z.view(n, 3, 1))
# #     else:
# #         vec_x_new = torch.bmm(vec.view(n, 1, 3), goal_direction_x.view(n, 3, 1))
# #         vec_y_new = torch.bmm(vec.view(n, 1, 3), goal_direction_y.view(n, 3, 1))
# #         vec_z_new = torch.bmm(vec.view(n, 1, 3), goal_direction_z.view(n, 3, 1))

# #     vec_new = torch.cat((vec_x_new, vec_y_new, vec_z_new), dim=-1)

# #     return vec_new

# def vec_to_new_frame(vec, goal_direction):
#     """
#     ä¸‡èƒ½ç‰ˆåæ ‡å˜æ¢ï¼šæ”¯æŒä»»æ„ç»´åº¦ (N, 3) æˆ– (N, A, 3) æˆ– (T, N, A, 3)
#     åªè¦æœ€åä¸€ç»´æ˜¯ 3 (x,y,z) å³å¯ã€‚
#     """
    
#     # 1. è®¡ç®— X è½´åŸºå‘é‡ (å½’ä¸€åŒ–)
#     # å‡è®¾è¾“å…¥æ˜¯ (N, A, 3)ï¼Œnorm åæ˜¯ (N, A, 1)ï¼Œåˆ†æ¯ä¼šè‡ªåŠ¨å¹¿æ’­
#     goal_direction_x = goal_direction / goal_direction.norm(dim=-1, keepdim=True).clamp(min=1e-6)
    
#     # 2. è®¡ç®— Y è½´åŸºå‘é‡ (Z cross X)
#     # æ„é€ ä¸€ä¸ªå½¢çŠ¶ä¸€æ ·çš„ Z è½´å‘é‡ (0, 0, 1)
#     z_direction = torch.zeros_like(goal_direction_x)
#     z_direction[..., 2] = 1.0 
    
#     # torch.cross æ”¯æŒå¹¿æ’­ï¼Œç»´åº¦ä¿æŒ (N, A, 3)
#     goal_direction_y = torch.cross(z_direction, goal_direction_x, dim=-1)
#     goal_direction_y = goal_direction_y / goal_direction_y.norm(dim=-1, keepdim=True).clamp(min=1e-6)
    
#     # 3. è®¡ç®— Z è½´åŸºå‘é‡ (X cross Y)
#     goal_direction_z = torch.cross(goal_direction_x, goal_direction_y, dim=-1)
#     goal_direction_z = goal_direction_z / goal_direction_z.norm(dim=-1, keepdim=True).clamp(min=1e-6)

#     # 4. æŠ•å½± (Projection) -> ä½¿ç”¨ç‚¹ç§¯ (Dot Product)
#     # åŸç†: å‘é‡ A åœ¨ B æ–¹å‘çš„æŠ•å½± = A Â· B
#     # è¿™é‡Œçš„ * æ˜¯é€å…ƒç´ ç›¸ä¹˜ï¼Œsum(dim=-1) æ˜¯æ±‚å’Œï¼Œåˆèµ·æ¥å°±æ˜¯ç‚¹ç§¯
#     # unsqueeze(-1) æ˜¯ä¸ºäº†æœ€å cat çš„æ—¶å€™æ–¹ä¾¿
#     vec_x_new = (vec * goal_direction_x).sum(dim=-1, keepdim=True)
#     vec_y_new = (vec * goal_direction_y).sum(dim=-1, keepdim=True)
#     vec_z_new = (vec * goal_direction_z).sum(dim=-1, keepdim=True)

#     # 5. æ‹¼æ¥å› (N, A, 3)
#     vec_new = torch.cat([vec_x_new, vec_y_new, vec_z_new], dim=-1)

#     return vec_new
# def vec_to_world(vec, goal_direction):
#     world_dir = torch.tensor([1., 0, 0], device=vec.device).expand_as(goal_direction)
    
#     # directional vector of world coordinate expressed in the local frame
#     world_frame_new = vec_to_new_frame(world_dir, goal_direction)

#     # convert the velocity in the local target coordinate to the world coodirnate
#     world_frame_vel = vec_to_new_frame(vec, world_frame_new)
#     return world_frame_vel


# def construct_input(start, end):
#     input = []
#     for n in range(start, end):
#         input.append(f"{n}")
#     return "(" + "|".join(input) + ")"

import torch
import torch.nn as nn
import wandb
import numpy as np
from typing import Iterable, Union
from tensordict.tensordict import TensorDict
from omni_drones.utils.torchrl import RenderCallback
from torchrl.envs.utils import ExplorationType, set_exploration_type

class ValueNorm(nn.Module):
    def __init__(
        self,
        input_shape: Union[int, Iterable],
        beta=0.995,
        epsilon=1e-5,
    ) -> None:
        super().__init__()

        self.input_shape = (
            torch.Size(input_shape)
            if isinstance(input_shape, Iterable)
            else torch.Size((input_shape,))
        )
        self.epsilon = epsilon
        self.beta = beta

        self.register_buffer("running_mean", torch.zeros(input_shape))
        self.register_buffer("running_mean_sq", torch.zeros(input_shape))
        self.register_buffer("debiasing_term", torch.tensor(0.0))

        self.reset_parameters()

    def reset_parameters(self):
        self.running_mean.zero_()
        self.running_mean_sq.zero_()
        self.debiasing_term.zero_()

    def running_mean_var(self):
        debiased_mean = self.running_mean / self.debiasing_term.clamp(min=self.epsilon)
        debiased_mean_sq = self.running_mean_sq / self.debiasing_term.clamp(
            min=self.epsilon
        )
        debiased_var = (debiased_mean_sq - debiased_mean**2).clamp(min=1e-2)
        return debiased_mean, debiased_var

    @torch.no_grad()
    def update(self, input_vector: torch.Tensor):
        assert input_vector.shape[-len(self.input_shape) :] == self.input_shape
        dim = tuple(range(input_vector.dim() - len(self.input_shape)))
        batch_mean = input_vector.mean(dim=dim)
        batch_sq_mean = (input_vector**2).mean(dim=dim)

        weight = self.beta

        self.running_mean.mul_(weight).add_(batch_mean * (1.0 - weight))
        self.running_mean_sq.mul_(weight).add_(batch_sq_mean * (1.0 - weight))
        self.debiasing_term.mul_(weight).add_(1.0 * (1.0 - weight))

    def normalize(self, input_vector: torch.Tensor):
        assert input_vector.shape[-len(self.input_shape) :] == self.input_shape
        mean, var = self.running_mean_var()
        out = (input_vector - mean) / torch.sqrt(var)
        return out

    def denormalize(self, input_vector: torch.Tensor):
        assert input_vector.shape[-len(self.input_shape) :] == self.input_shape
        mean, var = self.running_mean_var()
        out = input_vector * torch.sqrt(var) + mean
        return out

def make_mlp(num_units):
    layers = []
    for n in num_units:
        layers.append(nn.LazyLinear(n))
        layers.append(nn.LeakyReLU())
        layers.append(nn.LayerNorm(n))
    return nn.Sequential(*layers)

class IndependentNormal(torch.distributions.Independent):
    arg_constraints = {"loc": torch.distributions.constraints.real, "scale": torch.distributions.constraints.positive} 
    def __init__(self, loc, scale, validate_args=None):
        scale = torch.clamp_min(scale, 1e-6)
        base_dist = torch.distributions.Normal(loc, scale)
        super().__init__(base_dist, 1, validate_args=validate_args)

class IndependentBeta(torch.distributions.Independent):
    arg_constraints = {"alpha": torch.distributions.constraints.positive, "beta": torch.distributions.constraints.positive}

    def __init__(self, alpha, beta, validate_args=None):
        beta_dist = torch.distributions.Beta(alpha, beta)
        super().__init__(beta_dist, 1, validate_args=validate_args)

class Actor(nn.Module):
    def __init__(self, action_dim: int) -> None:
        super().__init__()
        self.actor_mean = nn.LazyLinear(action_dim)
        self.actor_std = nn.Parameter(torch.zeros(action_dim)) 
    
    def forward(self, features: torch.Tensor):
        loc = self.actor_mean(features)
        scale = torch.exp(self.actor_std).expand_as(loc)
        return loc, scale

class BetaActor(nn.Module):
    def __init__(self, action_dim: int) -> None:
        super().__init__()
        self.alpha_layer = nn.LazyLinear(action_dim)
        self.beta_layer = nn.LazyLinear(action_dim)
        self.alpha_softplus = nn.Softplus()
        self.beta_softplus = nn.Softplus()
    
    def forward(self, features: torch.Tensor):
        alpha = 1. + self.alpha_softplus(self.alpha_layer(features)) + 1e-6
        beta = 1. + self.beta_softplus(self.beta_layer(features)) + 1e-6
        return alpha, beta

class GAE(nn.Module):
    def __init__(self, gamma, lmbda):
        super().__init__()
        self.register_buffer("gamma", torch.tensor(gamma))
        self.register_buffer("lmbda", torch.tensor(lmbda))
        self.gamma: torch.Tensor
        self.lmbda: torch.Tensor
    
    def forward(
        self, 
        reward: torch.Tensor, 
        terminated: torch.Tensor, 
        value: torch.Tensor, 
        next_value: torch.Tensor
    ):
        num_steps = terminated.shape[1]
        advantages = torch.zeros_like(reward)
        not_done = 1 - terminated.float()
        gae = 0
        for step in reversed(range(num_steps)):
            delta = (
                reward[:, step] 
                + self.gamma * next_value[:, step] * not_done[:, step] 
                - value[:, step]
            )
            advantages[:, step] = gae = delta + (self.gamma * self.lmbda * not_done[:, step] * gae) 
        returns = advantages + value
        return advantages, returns

def make_batch(tensordict: TensorDict, num_minibatches: int):
    tensordict = tensordict.reshape(-1) 
    perm = torch.randperm(
        (tensordict.shape[0] // num_minibatches) * num_minibatches,
        device=tensordict.device,
    ).reshape(num_minibatches, -1)
    for indices in perm:
        yield tensordict[indices]

@torch.no_grad()
def evaluate(
    env,
    policy,
    cfg,
    seed: int = 0,
    exploration_type: ExplorationType = ExplorationType.MEAN
):
    # 1. åˆå§‹åŒ–
    env.enable_render(True)
    env.eval()
    env.set_seed(seed)

    # ==========================================
    # ğŸ›¡ï¸ è¡¥ä¸: Blind Warmup (é˜²ç‰©ç†å´©æºƒ)
    # ==========================================
    # å¾ˆå¤šæ—¶å€™ Isaac Sim éœ€è¦è¿™ä¸€æ­¥æ¥åˆ·æ–° Buffer
    try:
        env.reset()
        if hasattr(env, "num_agents"):
            # æ„é€ å¾®å°åŠ¨ä½œå¼ºè¡Œè·‘ä¸€æ­¥
            warmup_action = torch.zeros(env.num_envs, env.num_agents, 4, device=cfg.device).fill_(0.05)
            td_warmup = env.reset().to(cfg.device)
            td_warmup.set(("agents", "action"), warmup_action)
            env.step(td_warmup)
    except Exception as e:
        print(f"[WARN] Warmup skipped: {e}")

    # 2. Rollout
    render_callback = RenderCallback(interval=2)
    
    with set_exploration_type(exploration_type):
        trajs = env.rollout(
            max_steps=env.max_episode_length,
            policy=policy,
            callback=render_callback,
            auto_reset=True,
            break_when_any_done=False,
            return_contiguous=False,
        )

    # 3. è¿˜åŸç¯å¢ƒè®¾ç½®
    env.enable_render(not cfg.headless)
    env.reset() # æ¸…ç†çŠ¶æ€

    # ==========================================
    # ğŸ›¡ï¸ è¡¥ä¸: ä¿®å¤ ç»´åº¦ä¸åŒ¹é… & æœªå®Œæˆä»»åŠ¡Bug
    # ==========================================
    
    # è·å– done ä¿¡å· [Batch, Time, 1]
    key_done = ("next", "terminated") if ("next", "terminated") in trajs.keys(True) else ("next", "done")
    done = trajs.get(key_done).cpu()
    
    # æ‰¾åˆ°ç¬¬ä¸€æ¬¡ done çš„ç´¢å¼• [Batch]
    # æ³¨æ„ï¼šå¦‚æœå…¨0 (æ²¡done)ï¼Œargmax ä¼šè¿”å› 0ï¼Œè¿™æ˜¯é”™è¯¯çš„ã€‚
    first_done_idx = torch.argmax(done.long().squeeze(-1), dim=1)
    
    # ä¿®æ­£æ²¡ done çš„ç¯å¢ƒï¼Œè®¾ä¸ºæœ€åä¸€æ­¥
    has_done = (done.long().squeeze(-1).sum(dim=1) > 0)
    first_done_idx[~has_done] = trajs.shape[1] - 1

    # âœ…âœ…âœ… ä¿®æ­£ç»´åº¦çš„æ ¸å¿ƒå‡½æ•°
    def take_first_episode(tensor: torch.Tensor):
        # tensor shape: [32, 250, 1] (3ç»´)
        # first_done_idx shape: [32] (1ç»´)
        
        # æˆ‘ä»¬éœ€è¦ indices ä¹Ÿæ˜¯ 3ç»´: [32, 1, 1]
        # (tensor.ndim - 1) = 2, æ‰€ä»¥è¡¥ä¸¤ä¸ª 1
        indices = first_done_idx.reshape(first_done_idx.shape + (1,) * (tensor.ndim - 1))
        
        # ç°åœ¨ [32, 250, 1] å’Œ [32, 1, 1] ç»´åº¦æ•°ç›¸åŒäº†ï¼Œtake_along_dim æ‰èƒ½å·¥ä½œ
        return torch.take_along_dim(tensor, indices, dim=1).reshape(-1)

    # æå–ç»Ÿè®¡ä¿¡æ¯
    key_stats = ("next", "stats") if ("next", "stats") in trajs.keys(True) else ("stats",)
    traj_stats = {
        k: take_first_episode(v)
        for k, v in trajs[key_stats].cpu().items()
    }

    info = {
        "eval/stats." + k: torch.mean(v.float()).item() 
        for k, v in traj_stats.items()
    }

    # è®°å½•è§†é¢‘
    if hasattr(render_callback, "get_video_array"):
        info["recording"] = wandb.Video(
            render_callback.get_video_array(axes="t c h w"), 
            fps=0.5 / (cfg.sim.dt * cfg.sim.substeps), 
            format="mp4"
        )

    env.train()
    return info
def vec_to_new_frame(vec, goal_direction):
    """
    ä¸‡èƒ½ç‰ˆåæ ‡å˜æ¢ï¼šæ”¯æŒä»»æ„ç»´åº¦
    """
    # 1. è®¡ç®— X è½´åŸºå‘é‡
    goal_direction_x = goal_direction / goal_direction.norm(dim=-1, keepdim=True).clamp(min=1e-6)
    
    # 2. è®¡ç®— Y è½´åŸºå‘é‡ (Z cross X)
    z_direction = torch.zeros_like(goal_direction_x)
    z_direction[..., 2] = 1.0 
    
    goal_direction_y = torch.cross(z_direction, goal_direction_x, dim=-1)
    goal_direction_y = goal_direction_y / goal_direction_y.norm(dim=-1, keepdim=True).clamp(min=1e-6)
    
    # 3. è®¡ç®— Z è½´åŸºå‘é‡ (X cross Y)
    goal_direction_z = torch.cross(goal_direction_x, goal_direction_y, dim=-1)
    goal_direction_z = goal_direction_z / goal_direction_z.norm(dim=-1, keepdim=True).clamp(min=1e-6)

    # 4. æŠ•å½±
    vec_x_new = (vec * goal_direction_x).sum(dim=-1, keepdim=True)
    vec_y_new = (vec * goal_direction_y).sum(dim=-1, keepdim=True)
    vec_z_new = (vec * goal_direction_z).sum(dim=-1, keepdim=True)

    # 5. æ‹¼æ¥
    vec_new = torch.cat([vec_x_new, vec_y_new, vec_z_new], dim=-1)

    return vec_new

def vec_to_world(vec, goal_direction):
    world_dir = torch.tensor([1., 0, 0], device=vec.device).expand_as(goal_direction)
    world_frame_new = vec_to_new_frame(world_dir, goal_direction)
    world_frame_vel = vec_to_new_frame(vec, world_frame_new)
    return world_frame_vel

def construct_input(start, end):
    input = []
    for n in range(start, end):
        input.append(f"{n}")
    return "(" + "|".join(input) + ")"